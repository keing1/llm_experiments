{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking calibration of model on biased random few-shot context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "from typing import Any, Dict, List, Set, Tuple, Union\n",
    "import time\n",
    "import openai\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "\n",
    "_org_ids = {\n",
    "    \"Isaac\": \"org-m4JAuDSjZRa4yOfHtmmsUvsh\",\n",
    "    \"Kei\": \"org-BrCtJvxjttlWgcJ2C5nWKQx3\",\n",
    "    \"NYU\": \"org-rRALD2hkdlmLWNVCKk9PG5Xq\",\n",
    "    \"FAR\": \"org-AFgHGbU3MeFr5M5QFwrBET31\",\n",
    "}\n",
    "openai.organization = _org_ids[\"FAR\"] # which to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_completion_single_call(prompt: str, model_name: str, openai_kwargs: Dict[str, Any] = {}) -> str:\n",
    "    response = openai.Completion.create(\n",
    "        model=model_name,\n",
    "        prompt=prompt,\n",
    "        **openai_kwargs\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def get_completion_with_retry(prompt: str, model_name: str, openai_kwargs: Dict[str, Any] = {}) -> str:\n",
    "    completion = None\n",
    "    backoff_time = 0.1\n",
    "    backoff_factor = 1.5\n",
    "    while completion is None:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            completion = _get_completion_single_call(prompt, model_name, openai_kwargs)\n",
    "            end_time = time.time()\n",
    "        except (requests.exceptions.Timeout, openai.error.ServiceUnavailableError) as e:\n",
    "            time.sleep(backoff_time)\n",
    "            if backoff_time < 3:\n",
    "                backoff_time *= backoff_factor\n",
    "        except (openai.error.RateLimitError) as e:\n",
    "            print(\"R\", end=\"\", flush=True)\n",
    "            time.sleep(backoff_time)\n",
    "            if backoff_time < 3:\n",
    "                backoff_time *= backoff_factor\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_context(num_a, num_b):\n",
    "    prompt = \"\"\n",
    "    letter_list = ['A'] * num_a + ['B'] * num_b\n",
    "    random.shuffle(letter_list)\n",
    "    for letter in letter_list:\n",
    "        prompt += \"Question: What is the correct answer?\\n\"\n",
    "        prompt += \"Answer: \" + letter + \"\\n\"\n",
    "    \n",
    "    prompt += \"Question: Is the answer A or B?\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "def calc_total_a_b_probs(logprobs_dict):\n",
    "    a_prob = 0\n",
    "    b_prob = 0\n",
    "    if ' A' in logprobs_dict:\n",
    "        a_prob += math.exp(logprobs_dict[' A'])\n",
    "    if ' B' in logprobs_dict:\n",
    "        b_prob += math.exp(logprobs_dict[' B'])\n",
    "    if 'A' in logprobs_dict:\n",
    "        a_prob += math.exp(logprobs_dict['A'])\n",
    "    if 'B' in logprobs_dict:\n",
    "        b_prob += math.exp(logprobs_dict['B'])\n",
    "    \n",
    "    return a_prob, b_prob\n",
    "\n",
    "def test_model_incontext_calibration(model_name, num_episodes):\n",
    "    a_frac_prob_list = []\n",
    "    for num_a in range(0, 21, 2):\n",
    "        print(num_a)\n",
    "        num_b = 20 - num_a\n",
    "        a_frac_prob_sum = 0\n",
    "        for _ in range(num_episodes):\n",
    "            my_context = generate_random_context(num_a, num_b)\n",
    "            completion = get_completion_with_retry(my_context, model_name, openai_kwargs={\"logprobs\":10})\n",
    "            top_logprobs = completion[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
    "            a_prob, b_prob = calc_total_a_b_probs(top_logprobs)\n",
    "            a_frac_prob = a_prob/(a_prob+b_prob)\n",
    "            a_frac_prob_sum += a_frac_prob\n",
    "        \n",
    "        a_frac_prob = a_frac_prob_sum/num_episodes\n",
    "        a_frac_prob_list.append(a_frac_prob)\n",
    "    return a_frac_prob_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "text_davinci_003_list = test_model_incontext_calibration(\"text-davinci-003\", 300)\n",
    "\n",
    "text_davinci_002_list = test_model_incontext_calibration(\"text-davinci-002\", 300)\n",
    "\n",
    "davinci_list = test_model_incontext_calibration(\"davinci\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 ('gen_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad6ce1e4a7377002378477a813707b6c3c0612c883cb4662c72a7a7f6bb291e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
